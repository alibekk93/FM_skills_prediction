{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Preparation\n",
    "\n",
    "1. Merge separate files for each league and statistic type\n",
    "2. Calculate percentage above/below team average for each player statistic\n",
    "3. Aggregate original pre-transformation data per team for team-level statistics\n",
    "4. Apply Principal Component Analysis (PCA) to reduce the dimensionality of player statistics separately for each statistic group\n",
    "5. Apply PCA to reduce the dimensionality of team statistics\n",
    "6. Create tags to combine FBRef statistics with Football Manager and FC24 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change wd to repo home\n",
    "import os\n",
    "if os.getcwd().split('\\\\')[-1] == 'notebooks':\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.fm_functions import *\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Merge separate files for each league and statistic type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = '2022'\n",
    "\n",
    "# load FBRef data in separate dfs\n",
    "fbref_dfs = {filename : pd.read_csv(f'data/FBRef_split/{filename}', index_col=0)\n",
    "             for filename in os.listdir('data/FBRef_split')\n",
    "             if filename.split('_')[-1][:4] == season}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine FBRef dfs by type first\n",
    "table_types = ['defense', 'gca', 'keepersadv', 'keepers', 'misc', 'passingtypes',\n",
    "               'passing', 'playingtime', 'possession', 'shooting', 'stats']\n",
    "\n",
    "fbref_dfs_types = {}\n",
    "\n",
    "for t in table_types:\n",
    "    fbref_dfs_types[t] = {}\n",
    "    for k in fbref_dfs.keys():\n",
    "        if k.split('_')[1] == t:\n",
    "            fbref_dfs_types[t][k.split('_')[0]] = fbref_dfs[k]\n",
    "    try:\n",
    "        fbref_dfs_types[t] = pd.concat(fbref_dfs_types[t], axis=0, ignore_index=True).set_index(['_Player', '_Squad', '_Born'])\n",
    "    except:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine FBRef dfs into one\n",
    "fbref_combined_df = pd.concat(fbref_dfs_types, axis=1).reset_index()\n",
    "fbref_combined_df.columns = fbref_combined_df.columns.map(lambda x: '_'.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop keepers\n",
    "fbref_combined_df = fbref_combined_df[fbref_combined_df['stats__Pos']!='GK']\n",
    "# drop unknown YOB\n",
    "fbref_combined_df = fbref_combined_df.dropna(subset=['_Born_'])\n",
    "fbref_combined_df['yob'] = fbref_combined_df['_Born_'].astype('int')\n",
    "# drop players with less than 500 minutes\n",
    "fbref_combined_df = fbref_combined_df[fbref_combined_df['playingtime_PlayingTime_Min'] >= 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Calculate percentage above/below team average for each player statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_above_below(group):\n",
    "    return (group - group.mean()) / group.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbref_meandiff_df = fbref_combined_df.copy()\n",
    "\n",
    "for column_group in [defense_columns, touches_columns, passing_columns,\n",
    "                     progres_columns, attack_columns, misc_columns]:\n",
    "        fbref_meandiff_df[column_group] = fbref_meandiff_df.groupby('_Squad_')[column_group].transform(percent_above_below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Player_</th>\n",
       "      <th>_Squad_</th>\n",
       "      <th>_Born_</th>\n",
       "      <th>defense__Rk</th>\n",
       "      <th>defense__Nation</th>\n",
       "      <th>defense__Pos</th>\n",
       "      <th>defense__Age</th>\n",
       "      <th>defense__90s</th>\n",
       "      <th>defense_Tackles_Tkl</th>\n",
       "      <th>defense_Tackles_TklW</th>\n",
       "      <th>...</th>\n",
       "      <th>stats_Per90Minutes_G+A</th>\n",
       "      <th>stats_Per90Minutes_G-PK</th>\n",
       "      <th>stats_Per90Minutes_G+A-PK</th>\n",
       "      <th>stats_Per90Minutes_xG</th>\n",
       "      <th>stats_Per90Minutes_xAG</th>\n",
       "      <th>stats_Per90Minutes_xG+xAG</th>\n",
       "      <th>stats_Per90Minutes_npxG</th>\n",
       "      <th>stats_Per90Minutes_npxG+xAG</th>\n",
       "      <th>stats_Matches</th>\n",
       "      <th>yob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sargis Adamyan</td>\n",
       "      <td>Köln</td>\n",
       "      <td>1993</td>\n",
       "      <td>2.0</td>\n",
       "      <td>am ARM</td>\n",
       "      <td>FWMF</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>-0.573604</td>\n",
       "      <td>-0.646067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.58</td>\n",
       "      <td>Matches</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karim Adeyemi</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>2002</td>\n",
       "      <td>3.0</td>\n",
       "      <td>de GER</td>\n",
       "      <td>FWMF</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>-0.443223</td>\n",
       "      <td>-0.334395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.52</td>\n",
       "      <td>Matches</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amine Adli</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>2000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ma MAR</td>\n",
       "      <td>FWMF</td>\n",
       "      <td>22.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>-0.331878</td>\n",
       "      <td>-0.348659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>Matches</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naouirou Ahamada</td>\n",
       "      <td>Stuttgart</td>\n",
       "      <td>2002</td>\n",
       "      <td>6.0</td>\n",
       "      <td>fr FRA</td>\n",
       "      <td>MF</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>-0.404814</td>\n",
       "      <td>-0.338346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.13</td>\n",
       "      <td>Matches</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ludovic Ajorque</td>\n",
       "      <td>Mainz 05</td>\n",
       "      <td>1994</td>\n",
       "      <td>7.0</td>\n",
       "      <td>fr FRA</td>\n",
       "      <td>FWMF</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-0.788104</td>\n",
       "      <td>-0.716418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>Matches</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           _Player_     _Squad_  _Born_  defense__Rk defense__Nation  \\\n",
       "1    Sargis Adamyan        Köln    1993          2.0          am ARM   \n",
       "2     Karim Adeyemi    Dortmund    2002          3.0          de GER   \n",
       "3        Amine Adli  Leverkusen    2000          4.0          ma MAR   \n",
       "5  Naouirou Ahamada   Stuttgart    2002          6.0          fr FRA   \n",
       "6   Ludovic Ajorque    Mainz 05    1994          7.0          fr FRA   \n",
       "\n",
       "  defense__Pos  defense__Age  defense__90s  defense_Tackles_Tkl  \\\n",
       "1         FWMF          29.0           7.2            -0.573604   \n",
       "2         FWMF          20.0          15.4            -0.443223   \n",
       "3         FWMF          22.0          15.9            -0.331878   \n",
       "5           MF          20.0          15.5            -0.404814   \n",
       "6         FWMF          28.0          13.5            -0.788104   \n",
       "\n",
       "   defense_Tackles_TklW  ...  stats_Per90Minutes_G+A  stats_Per90Minutes_G-PK  \\\n",
       "1             -0.646067  ...                    0.42                     0.14   \n",
       "2             -0.334395  ...                    0.71                     0.39   \n",
       "3             -0.348659  ...                    0.50                     0.31   \n",
       "5             -0.338346  ...                    0.26                     0.13   \n",
       "6             -0.716418  ...                    0.52                     0.45   \n",
       "\n",
       "   stats_Per90Minutes_G+A-PK  stats_Per90Minutes_xG  stats_Per90Minutes_xAG  \\\n",
       "1                       0.42                   0.47                    0.11   \n",
       "2                       0.71                   0.33                    0.24   \n",
       "3                       0.50                   0.31                    0.11   \n",
       "5                       0.26                   0.05                    0.08   \n",
       "6                       0.52                   0.34                    0.13   \n",
       "\n",
       "   stats_Per90Minutes_xG+xAG  stats_Per90Minutes_npxG  \\\n",
       "1                       0.58                     0.47   \n",
       "2                       0.57                     0.28   \n",
       "3                       0.42                     0.31   \n",
       "5                       0.13                     0.05   \n",
       "6                       0.47                     0.34   \n",
       "\n",
       "   stats_Per90Minutes_npxG+xAG  stats_Matches   yob  \n",
       "1                         0.58        Matches  1993  \n",
       "2                         0.52        Matches  2002  \n",
       "3                         0.42        Matches  2000  \n",
       "5                         0.13        Matches  2002  \n",
       "6                         0.47        Matches  1994  \n",
       "\n",
       "[5 rows x 287 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbref_meandiff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['shooting_Standard_Dist'], dtype='object')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbref_meandiff_df[all_stats_columns].columns[fbref_meandiff_df[all_stats_columns].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbref_meandiff_df['shooting_Standard_Dist'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbref_meandiff_df.dropna(subset=['shooting_Standard_Dist'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbref_meandiff_df = fbref_meandiff_df[['_Player_', '_Born_', '_Squad_'] + all_stats_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbref_meandiff_df.to_csv(f'data/{season}_FBRef_meandiff.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Aggregate original pre-transformation data per team for team-level statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats_df = fbref_combined_df.groupby('_Squad_')[all_stats_columns].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_stats_df.columns[team_stats_df.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats_df.to_csv(f'data/{season}_FBRef_teams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Apply PCA to reduce the dimensionality of player statistics separately for each statistic group:\n",
    "- defense\n",
    "- touches\n",
    "- passing\n",
    "- progression\n",
    "- attack\n",
    "- miscelaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfbref_meandiff_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattack_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alibe\\miniconda3\\envs\\fbref\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\alibe\\miniconda3\\envs\\fbref\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alibe\\miniconda3\\envs\\fbref\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:460\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    439\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alibe\\miniconda3\\envs\\fbref\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:483\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA does not support sparse input. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncatedSVD for a possible alternative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m     )\n\u001b[1;32m--> 483\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alibe\\miniconda3\\envs\\fbref\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\alibe\\miniconda3\\envs\\fbref\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\alibe\\miniconda3\\envs\\fbref\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alibe\\miniconda3\\envs\\fbref\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "pca.fit_transform(fbref_meandiff_df[attack_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62728758, 0.23231409])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbref_pca_df = \n",
    "\n",
    "for column_group in [defense_columns, touches_columns, passing_columns,\n",
    "                     progres_columns, attack_columns, misc_columns]:\n",
    "        fbref_pca_df[column_group] = fbref_pca_df.groupby('_Squad_')[column_group].transform(percent_above_below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine with FM and FC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FM data\n",
    "FM_data = pd.read_csv('./data/FM 2023.csv')\n",
    "FM_data['dob'] = pd.to_datetime(FM_data['Date of birth'])\n",
    "FM_data['yob'] = FM_data['dob'].dt.year\n",
    "# load FC data\n",
    "FC_data = pd.read_csv('./data/FC24.csv')\n",
    "FC_data = FC_data[FC_data['update_as_of']=='2022-09-26']\n",
    "FC_data['dob'] = pd.to_datetime(FC_data['dob'])\n",
    "FC_data['yob'] = FC_data['dob'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbref_combined_outfielders_grouped = fbref_combined_outfielders_df.groupby('yob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8452/8452 [02:04<00:00, 68.16it/s] \n"
     ]
    }
   ],
   "source": [
    "FM_data['name_match'] = FM_data.progress_apply(lambda row: get_best_match(\n",
    "    row['Name'], row['yob'], grouped_df=fbref_combined_outfielders_grouped,\n",
    "    player_col='_Player_', threshold=80), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18533/18533 [06:41<00:00, 46.20it/s] \n"
     ]
    }
   ],
   "source": [
    "FC_data['name_match'] = FC_data.progress_apply(lambda row: get_best_match(\n",
    "    row['long_name'], row['yob'], grouped_df=fbref_combined_outfielders_grouped,\n",
    "    player_col='_Player_', threshold=80), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine FM and FC dataframes with FBRef\n",
    "FM_fbref_combined_df = fbref_combined_outfielders_df.merge(right=FM_data, left_on=['_Player_', '_Born_'], right_on=['name_match', 'yob'], how='inner')\n",
    "FC_fbref_combined_df = fbref_combined_outfielders_df.merge(right=FC_data, left_on=['_Player_', '_Born_'], right_on=['name_match', 'yob'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_fbref_combined_df = FM_fbref_combined_df[['_Player_'] + all_stats_columns + FM_columns]\n",
    "FC_fbref_combined_df = FC_fbref_combined_df[['_Player_'] + all_stats_columns + FC_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the combined files\n",
    "FM_fbref_combined_df.to_csv(f'data/FM_fbref_combined_data_{season}.csv')\n",
    "FC_fbref_combined_df.to_csv(f'data/FC_fbref_combined_data_{season}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fbref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
